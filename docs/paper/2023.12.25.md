## 近期工作：

1. 进行有关骨架序列数据mixup的论文阅读
2. 继续进行代码复现
下方是本周阅读的关于MAE的总结要点

---

#### 【CVPR2021】Masked Autoencoders Are Scalable Vision Learners

####  创新方法概述：
提出一种可扩展的自监督学习方法：MAE（Masked Autoencoders，掩膜自编码器）。MAE的做法是：随机遮盖输入图片的子块，然后重建丢失像素。其核心设计为：
 
<strong>1. 非对称的编码-解码架构 ：</strong>

编码器的输入为没有被mask的子块；解码器为轻量级（解码器仅在图像重建的预训练中起作用，因此解码器设计可以独立于编码器，且灵活和轻量级），输入为编码器的输出和被mask部分的位置信息，输出为待重建的丢失像素的值。 

**2. 高比例mask情况下的自监督：**

  高比例指75%



通过以上两点，可**加速模型训练**（3倍或以上，因为编码器只输入未被mask的部分，数据量少）、**提高准确率、泛化性好**（因其可扩展而可训大模型）。仅使用 **ImageNet-1K 数据**，vanilla **ViT-Huge 模型**实现了最佳准确率 (87.8%，超过了之前所有只使用ImageNet-1K数据的结果)。并且其下游任务的迁移也优于监督训练，证明可扩展能力很可观。 
    


#### （一）Preliminaries

- **Autoencoding(自编码器)：** 是一种无监督学习算法，在 深度学习 领域中被广泛应用。 它可以通过将输入数据进行编码和解码来重构数据，从而学习数据的低维表示。 自动编码器可以用于特征提取、降维和数据重建等任务。
- **Masked image encoding：** 从被屏蔽破坏的图像中学习表示


#### （二）Methodology
MAE是一种非常简单的自编码器方案：基于给定部分观测信息对原始信号进行重建 。类似于其他自编码器，所提MAE包含一个将观测信号映射为隐式表达的编码器，一个用于将隐式表达重建为原始信号的解码器:
> - 与经典自编码器不同之处在于：我们采用了**非对称设计**，这使得编码器仅依赖于部分观测信息(无需掩码token信息)
> - 而轻量解码器则接与所得**隐式表达与掩码token进行原始信号重建**：

![Alt text](image-7.png)

- **基本流程：**
    - 对图片切分 patch， 随机挑选少部分（比如文中25%）作为网络输入；
    - 输入通过 encoder 得到对应编码后的 encoded patches
    - 将 encoded patches 还原到对应的原始位置，并在缺失的部分补上 masked patches
    - 送入 decoder， 每个 decoder 预测对应 patch 的图像像素点；
    - 计算预测的像素和原始图片的像素之间 MSE 作为 loss。
    - 取训练完的模型的 encoder 部分作为下游任务的 basemodel 并在下游任务下 finetune。

##### 1.Masking  
参考ViT，我们将输入图像拆分为非重叠块，然后采样一部分块并移除其余块(即Mask)。我们的采样策略非常简单：服从均匀分布的无重复随机采样 。我们将该采样策略称之为“随机采样”。具有高掩码比例的随机采样可以极大程度消除冗余，进而构建一个不会轻易的被近邻块推理解决的任务 (可参考下面图示)。而均匀分布则避免了潜在的中心偏置问题。
##### 2.MAE Encoder  
MAE中的编码器是一种ViT，但仅作用于可见的未被Mask的块。类似于标准ViT，该编码器通过线性投影于位置嵌入对块进行编码，然后通过一系列Transformer模块进行处理。然而，由于该编解码仅在较小子集块(比如25%)进行处理，且未用到掩码Token信息。这就使得我们可以训练一个非常大的编码器 。

##### 3.MAE Decoder  
MAE解码器的输入包含：(1) 编码器的输出；(2) 掩码token。正如Figure1所示，每个掩码Token共享的可学习向量，它用于指示待预测遗失块。此时，我们对所有token添加位置嵌入信息。解码器同样包含一系列Transformer模块。

>注：MAE解码器仅在预训练阶段用于图像重建，编码器则用来生成用于识别的图像表达 。因此，解码器的设计可以独立于编码设计，具有高度的灵活性。在实验过程中，我们采用了窄而浅的极小解码器，比如默认解码器中每个token的计算量小于编码器的10% 。通过这种非对称设计，token的全集仅被轻量解码器处理，大幅减少了预训练时间。

##### 4.Reconstruction target  
该MAE通过预测每个掩码块的像素值进行原始信息重建 。解码器的最后一层为线性投影，其输出通道数等于每个块的像素数量。编码器的输出将通过reshape构建重建图像。损失函数则采用了MSE，注：类似于BERT仅在掩码块计算损失。

##### 5.Simple implementation  
MAE预训练极为高效，更重要的是：它不需要任何特定的稀疏操作。实现过程可描述如下：
> - 首先，我们通过线性投影与位置嵌入对每个输入块生成token；
> - 然后，我们随机置换(random shuffle)token序列并根据掩码比例移除最后一部分token；
> - 其次，完成编码后，我们在编码块中插入掩码token并反置换(unshuffle)得到全序列token以便于与target进行对齐；
> - 最后，我们将解码器作用于上述全序列token。

正如上所述：MAE无需稀疏操作。此外，shuffle与unshuffle操作非常快，引入的计算量可以忽略。


#### （三）Experiment
1. **Baseline: ViT-Large**
  我们采用ViT-Large作为消融实验的骨干，上表为从头开始训练与MAE微调的性能对比。可以看到：从头开始训练(200epoch)，ViT-L的性能为82.5%且无需强正则技术；而MAE(注：仅微调50epoch)则取得了大幅性能提升。
![Alt text](image.png)
![Alt text](image-2.png)
2. **Decoder Design**
  从Table1(a)与Table1(b)可以看到：解码器的设计可以非常灵活 。总而言之，默认解码器非常轻量，仅有8个模块，维度为512，每个token的计算量仅为编码的9%。

    > - Decoder depth：decoder 的深度对 linear probing 影响很大， 因为 decoder 越深，那么训练时过程中会使它的浅层对应的特征与预训练的重建任务越无关，也就是越抽象，利于迁移。而 finetune 因为会在新任务上重新学习，所以深度对他影响要小很多。
    > - Decoder width：decoder的宽度对 linear probing 影响很大， 对finetune 影响很小。

    所以作者最后的模型选择 finetune 方式迁移到下游任务，且采用比较浅比较窄的 decoder(8 个 blocks， 512-d width)， 这样能够加速且省显存。

3. **Mask Token**  
MAE的的重要设计： 在编码阶段跳过掩码token，在解码阶段对其进行处理。Table1(c)给出了性能对比，可以看到：编码器使用掩码token则会导致性能下降 。

    > - encoder 是否使用 mask token：文中取可见 patches 的方式是对输入的所有 patch （假设一共有 N 个patch）先 shuffle 一下， 然后把后 N * masking_ratio 的 patch 丢掉，只有前面部分过网络。
    > - 另一种可能的方法是在 encoder 那增加一个 mask token 指代哪些 patch 需要保留哪些不需要保留。 实验结果表明这样做效果明显变差了， 原因使用 mask encoding 在训练时模型见到的是一些残缺的输入， 但是在实际下游任务中见到的确实完整的输入， 这会带来很大的 gap。

4. **Recontruction target**
 重建目标 Table1(d)比较了不同重建目标的性能：1. 直接对原始像素重建；2. 以 patch 内像素的统计量对 patch 做 norm 后的像素重建；3.PCA 后重建； 4. dVAE。可以看到：引入块归一化可以进一步提升模型精度 。

5. **Data Augmentation 数据增广**  
   Table1(e)比较了不同数据增广的影响，可以看到：MAE仅需crop即可表现非常好，添加ColorJitter反而会影响性能 。另外，令人惊讶的是：当不使用数据增广时，MAE性能也非常优秀 。

6. **Mask Sampling**  
    Table1(f)比较了不同掩码随机采样策略，可以看到：不同的采样策略均具有比较好的性能，而随机采样则具有最佳性能 。

7. **Masking  ratio**
   下图给出了掩码比例的影响，可以看到：最优比例惊人的高 。掩码比例为75%有益于两种监督训练方式(端到端微调与linear probing)。这与BERT的行为截然相反，其掩码比例为15%。
   ![Alt text](image-1.png)

8. **Training Schedule**
    下图给出了不同训练机制的性能对比(此时采用了800epoch预训练)，可以看到：更长的训练可以带来更定的精度提升 。作者还提到：哪怕1600epoch训练也并未发现linear probing方式的性能饱和。这与MoCoV3中的300epoch训练饱和截然相反 ：在每个epoch，MAE仅能看到25%的图像块；而MoCoV3则可以看到200%，甚至更多的图像块。
    ![Alt text](image-3.png)


#### （四）Comparisons with Previous Results
![Alt text](image-4.png)

上表给出了所提MAE与其他自监督方案的性能对比，从中可以看到：

 > - 对于ViT-B来说，不同方案的性能非常接近；对于ViT-L来说，不同方案的性能差异则变大。这意味着：对更大模型降低过拟合更具挑战性 。
 > - MAE可以轻易的扩展到更大模型并具有稳定的性能提升。比如：ViT-H取得了86.9%的精度，在448尺寸微调后，性能达到了87.8% ，超越了此前VOLO的最佳87.1%(尺寸为512)。注：该结果仅使用了ViT，更优秀的网络表达可能会更好。

#### （五）Transfer Learning Experiments

![Alt text](image-5.png)
上表给出了COCO检测与分割任务上的迁移性能对比，可以看到：相比监督预训练，MAE取得了全配置最佳 。当骨干为ViT-B时，MAE可以取得2.4AP指标提升；当骨干为ViT-L时，性能提升达4.0AP。

![Alt text](image-6.png)
上表给出了ADE20K语义分割任务上的迁移性能对比，可以看到：MAE可以大幅改善ViT-L的性能，比监督训练高3.7。


#### （六）Conclusion
本文提出了一种简单高效的视觉自监督训练方法，作者从图像和语言的信号本质特征出发，考虑通过使用NLP领域中较为成熟的技术提高视觉自监督框架的性能，同时认真分析和处理二者之间的差距。本文也观察到，所提的MAE方法可以从局部推断出复杂的整体重建，表明它已经学习到了许多视觉概念，即语义。本文也提醒我们在视觉Transfromer火热的当下，我们更应该从特征学习的本质出发来思考视觉学习的问题。“扩展性好的简单算法是深度学习的核心”。